{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(\"Importing misc libraries\")\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Updating sys.path\")\n",
    "project_root = Path.cwd().parent.parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "models_path = Path.cwd().parent\n",
    "if str(models_path) not in sys.path:\n",
    "    sys.path.append(str(models_path))\n",
    "    \n",
    "print(\"Importing torch libraries\")\n",
    "from torch.utils.data import DataLoader\n",
    "import multiprocessing\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.amp import autocast, GradScaler\n",
    "    \n",
    "print(\"Importing unet lib\")\n",
    "import importlib\n",
    "import unet.Unet as u\n",
    "importlib.reload(u)\n",
    "\n",
    "print(\"Importing dataset lib\")\n",
    "from dataset.IntersectionDataset import IntersectionDataset, IntersectionDataset2, IntersectionDatasetClasses, custom_collate_fn\n",
    "import loss.loss_lib as ll\n",
    "importlib.reload(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "img = cv2.imread(\"../unet/satellite.png\")\n",
    "img_t = torch.tensor(img).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
    "model = u.UNet(n_channels=3, n_classes=1).to(device)\n",
    "output = model(img_t)\n",
    "print(\"Output shape:\", output.shape)  # Expected: torch.Size([1, 1, 400, 400])\n",
    "#u.display_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = u.UNet(n_channels=3, n_classes=1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"../../dataset/dataset/train\"\n",
    "img_transform = ToTensor()\n",
    "path_transform = ToTensor()\n",
    "dataset_train = IntersectionDataset(root_dir=dataset_dir,\n",
    "                              transform=img_transform,\n",
    "                              path_transform=path_transform)\n",
    "\n",
    "dataset_dir = \"../../dataset/dataset/test\"\n",
    "img_transform = ToTensor()\n",
    "path_transform = ToTensor()\n",
    "dataset_test = IntersectionDataset(root_dir=dataset_dir,\n",
    "                              transform=img_transform,\n",
    "                              path_transform=path_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(len(dataset_train))\n",
    "print(len(dataset_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = multiprocessing.cpu_count()\n",
    "b = 4\n",
    "\n",
    "# split dataset into train and test\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# test_size = len(dataset) - train_size\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=b, shuffle=True, num_workers=num_workers)\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=b, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=1, eta_min=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "lc = ll.CmapLoss().to(device)\n",
    "lb = ll.BCELoss()\n",
    "\n",
    "def total_loss(output, target, alpha = 0.5):\n",
    "    loss = alpha * lc(output, target) + (1 - alpha) * lb(output, target)\n",
    "    return loss.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 500\n",
    "alpha = 0.5\n",
    "epochs = tqdm.tqdm(range(n_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = torch.nn.BCEWithLogitsLoss()\n",
    "lb = lb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in epochs:\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        satellite = batch[\"satellite\"].to(device)\n",
    "        path_line = batch[\"path_line\"].to(device)\n",
    "        cold_map = batch[\"cold_map\"].to(device)\n",
    "        cmap_f = torch.flatten(cold_map)\n",
    "        output = model(satellite)\n",
    "        L_cmap = lc(cmap_f, output)\n",
    "        L_bce = lb(output, path_line)\n",
    "        loss = alpha * L_cmap + (1 - alpha) * L_bce\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.display_output(output, threshold=0.5, thresholded=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[0].shape, output.max(), output.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "ckpts = sorted([f for f in os.listdir(\"ckpt\") if f.startswith(\"bce_new\") and f.endswith(\".pth\")])\n",
    "print(ckpts)\n",
    "ckpt = torch.load(\"ckpt/checkpoint_epoch_1000.pth\")\n",
    "\n",
    "#model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "#model.eval()\n",
    "\n",
    "rs = [r for r in random.sample(range(len(dataset_test)), 6)]\n",
    "print(rs)\n",
    "\n",
    "cols = 6\n",
    "rows = 2\n",
    "fig = plt.figure(figsize=(24, 8))\n",
    "i = 1\n",
    "c = 0\n",
    "\n",
    "for ck in ckpts:\n",
    "    i = 1\n",
    "    cnt = 0\n",
    "    \n",
    "    cols = 6\n",
    "    rows = 2\n",
    "    fig = plt.figure(figsize=(24, 8))\n",
    "    \n",
    "    c = torch.load(\"./ckpt/checkpoint_epoch_100.pth\")\n",
    "    \n",
    "    model2 = u.UNet(n_channels=3, n_classes=1).to(device)\n",
    "    model2.load_state_dict(c[\"model_state_dict\"])\n",
    "    model2.eval()\n",
    "    \n",
    "    while i <= rows * cols:\n",
    "        r = random.randint(0, len(dataset_test))\n",
    "        sat = dataset_test[rs[cnt]][\"satellite\"].permute(1, 2, 0)\n",
    "        fig.add_subplot(rows, cols, i)\n",
    "        plt.imshow(sat)\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        fig.add_subplot(rows, cols, i)\n",
    "        \n",
    "        s = dataset_test[rs[cnt]][\"satellite\"].to(device)\n",
    "        #print(s.shape)\n",
    "        s = s.unsqueeze(0)\n",
    "        output = model2(s)\n",
    "        output = torch.sigmoid(output)\n",
    "        \n",
    "        output = output.squeeze(0).squeeze(0).detach().cpu().numpy()\n",
    "        \n",
    "        plt.imshow(output, cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        i += 1\n",
    "        cnt += 1\n",
    "    \n",
    "    fig.suptitle(f\"{ck}\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    #fig.savefig(f\"ckpt/{ck}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    break\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add statistics collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = u.UNet(n_channels=3, n_classes=1).to(device)\n",
    "\n",
    "del(optimizer)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "del(scheduler)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=1, eta_min=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "n_epochs = 200\n",
    "alpha = 0.5\n",
    "epochs = tqdm.tqdm(range(n_epochs))\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "for epoch in epochs:\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    running_train_correct = 0\n",
    "    running_train_total = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        satellite = batch[\"satellite\"].to(device)\n",
    "        path_line = batch[\"path_line\"].to(device)\n",
    "        path_line_ee = batch[\"ee_data\"]\n",
    "        #cold_map = batch[\"cold_map\"].to(device)\n",
    "        #cmap_f = torch.flatten(cold_map)\n",
    "        output = model(satellite)\n",
    "        #L_cmap = lc(cmap_f, output)\n",
    "        L_bce = lb(output, path_line)\n",
    "        #loss = alpha * L_cmap + (1 - alpha) * L_bce\n",
    "        loss = L_bce\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_train_loss += loss.item()\n",
    "        \n",
    "        p = torch.sigmoid(output)\n",
    "        p = (p > 0.5).float()\n",
    "        running_train_correct += (p == path_line).sum().item()\n",
    "        running_train_total += path_line.numel()\n",
    "        \n",
    "    avg_train_loss = running_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracy = running_train_correct / running_train_total\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    running_test_correct = 0\n",
    "    running_test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            satellite = batch[\"satellite\"].to(device)\n",
    "            path_line = batch[\"path_line\"].to(device)\n",
    "            path_line_ee = batch[\"ee_data\"]\n",
    "\n",
    "            #cold_map = batch[\"cold_map\"].to(device)\n",
    "            #cmap_f = torch.flatten(cold_map)\n",
    "            output = model(satellite)\n",
    "            #L_cmap = lc(cmap_f, output)\n",
    "            L_bce = lb(output, path_line)\n",
    "            #loss = alpha * L_cmap + (1 - alpha) * L_bce\n",
    "            loss = L_bce\n",
    "            \n",
    "            running_test_loss += loss.item()\n",
    "            \n",
    "            p = torch.sigmoid(output)\n",
    "            p = (p > 0.5).float()\n",
    "            running_test_correct += (p == path_line).sum().item()\n",
    "            running_test_total += path_line.numel()\n",
    "            \n",
    "    avg_test_loss = running_test_loss / len(test_dataloader)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    \n",
    "    test_accuracy = running_test_correct / running_test_total\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'test_losses': test_losses,\n",
    "            'train_accuracies': train_accuracies,\n",
    "            'test_accuracies': test_accuracies,\n",
    "        }\n",
    "        os.makedirs('./ckpt', exist_ok=True)\n",
    "        torch.save(checkpoint, f'./ckpt/bce_new_dataset_checkpoint_epoch_{epoch + 1}.pth')\n",
    "    \n",
    "    \n",
    "    #print(f\"Epoch {epoch+1}/{n_epochs} - Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f}\")\n",
    "    \n",
    "torch.save(model.state_dict(), \"model_200e_bce_new_dataset.pth\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss graphs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, n_epochs + 1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, n_epochs + 1), test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Evaluation Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the accuracy graphs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, n_epochs + 1), train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(range(1, n_epochs + 1), test_accuracies, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Evaluation Accuracy Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_exit_to_class(exit_x, exit_y):\n",
    "    if exit_y == 0:\n",
    "        return 1  # left\n",
    "    elif exit_y == 399:\n",
    "        return 2  # right\n",
    "    elif exit_x == 0:\n",
    "        return 3  # ahead\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected exit position\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) and m.kernel_size == (1, 1):\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.001)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"../../dataset/dataset/train\"\n",
    "img_transform = ToTensor()\n",
    "path_transform = ToTensor()\n",
    "dataset_train = IntersectionDataset(root_dir=dataset_dir, \n",
    "                                    transform=img_transform,\n",
    "                                    path_transform=path_transform)\n",
    "\n",
    "dataset_dir = \"../../dataset/dataset/test\"\n",
    "img_transform = ToTensor()\n",
    "path_transform = ToTensor()\n",
    "dataset_test = IntersectionDataset(root_dir=dataset_dir,\n",
    "                                   transform=img_transform,\n",
    "                                   path_transform=path_transform)\n",
    "\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "b = 4\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=b, shuffle=True, num_workers=num_workers)\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=b, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    del(model)\n",
    "except NameError:\n",
    "    pass\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = u.UNet(n_channels=3, n_classes=5).to(device) # background, left, right, ahead, stacked\n",
    "model.apply(init_weights)\n",
    "\n",
    "try:\n",
    "    del(optimizer)\n",
    "except NameError:\n",
    "    pass\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "try:\n",
    "    del(scheduler)\n",
    "except NameError:\n",
    "    pass\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=1, eta_min=1e-5)\n",
    "\n",
    "lb = torch.nn.CrossEntropyLoss()\n",
    "lb = lb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "n_epochs = 10\n",
    "alpha = 0.5\n",
    "epochs = tqdm.tqdm(range(n_epochs), desc=\"Training\")\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "for epoch in epochs:\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    running_train_correct = 0\n",
    "    running_train_total = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        satellite = batch[\"satellite\"].to(device)\n",
    "        path_line = batch[\"path_line\"].to(device)\n",
    "        path_line_ee = batch[\"ee_data\"]\n",
    "        \n",
    "        B, _, H, W = path_line.shape\n",
    "        target = torch.full((B, H, W), 0, dtype=torch.long, device=device)\n",
    "        \n",
    "        for i in range(B):\n",
    "            exit_x = path_line_ee[\"exit\"][\"x\"][i].item()\n",
    "            exit_y = path_line_ee[\"exit\"][\"y\"][i].item()\n",
    "            \n",
    "            class_label = map_exit_to_class(exit_x, exit_y)\n",
    "            \n",
    "            mask = path_line[i, 0] > 0\n",
    "            \n",
    "            \n",
    "            target[i, mask] = class_label\n",
    "        \n",
    "        output = model(satellite)\n",
    "        L_ce = lb(output, target)\n",
    "        loss = L_ce\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_train_loss += loss.item()\n",
    "        \n",
    "        p = torch.argmax(output, dim=1)\n",
    "        valid_mask = target != 0\n",
    "        running_train_correct += (p[valid_mask] == target[valid_mask]).sum().item()\n",
    "        running_train_total += valid_mask.sum().item()\n",
    "        \n",
    "    avg_train_loss = running_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracy = running_train_correct / running_train_total\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    running_test_correct = 0\n",
    "    running_test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            satellite = batch[\"satellite\"].to(device)\n",
    "            path_line = batch[\"path_line\"].to(device)\n",
    "            path_line_ee = batch[\"ee_data\"]\n",
    "\n",
    "            B, _, H, W = path_line.shape\n",
    "            target = torch.full((B, H, W), 0, dtype=torch.long, device=device)\n",
    "            \n",
    "            for i in range(B):\n",
    "                exit_x = path_line_ee[\"exit\"][\"x\"][i].item()\n",
    "                exit_y = path_line_ee[\"exit\"][\"y\"][i].item()\n",
    "                \n",
    "                class_label = map_exit_to_class(exit_x, exit_y)\n",
    "                \n",
    "                mask = path_line[i, 0] > 0\n",
    "                \n",
    "                target[i, mask] = class_label\n",
    "            \n",
    "            output = model(satellite)\n",
    "            L_ce = lb(output, target)\n",
    "            loss = L_ce\n",
    "            \n",
    "            running_test_loss += loss.item()\n",
    "            \n",
    "            p = torch.argmax(output, dim=1)\n",
    "            valid_mask = target != 0\n",
    "            running_test_correct += (p[valid_mask] == target[valid_mask]).sum().item()\n",
    "            running_test_total += valid_mask.sum().item()\n",
    "            \n",
    "    avg_test_loss = running_test_loss / len(test_dataloader)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    \n",
    "    test_accuracy = running_test_correct / running_test_total\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    scheduler.step()  \n",
    "    \n",
    "    if (epoch + 1) % 25 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'test_losses': test_losses,\n",
    "            'train_accuracies': train_accuracies,\n",
    "            'test_accuracies': test_accuracies,\n",
    "        }\n",
    "        os.makedirs('./ckpt', exist_ok=True)\n",
    "        torch.save(checkpoint, f'./ckpt/bce_checkpoint_epoch_{epoch + 1}_4classes.pth')\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} - Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f}\")\n",
    "    \n",
    "    \n",
    "#torch.save(model.state_dict(), \"model_200e_ce_new_dataset_3class.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "n_epochs = 10\n",
    "alpha = 0.5\n",
    "epochs = tqdm.tqdm(range(n_epochs))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "for batch in train_dataloader:\n",
    "    satellite = batch[\"satellite\"].to(device)\n",
    "    path_line = batch[\"path_line\"].to(device)\n",
    "    path_line_ee = batch[\"ee_data\"]\n",
    "    \n",
    "    l = len(batch) -1\n",
    "    \n",
    "    B, _, H, W = path_line.shape\n",
    "    target = torch.full((B, H, W), 0, dtype=torch.long, device=device)\n",
    "    \n",
    "    for i in range(B):\n",
    "        exit_x = path_line_ee[\"exit\"][\"x\"][i].item()\n",
    "        exit_y = path_line_ee[\"exit\"][\"y\"][i].item()\n",
    "        \n",
    "        class_label = map_exit_to_class(exit_x, exit_y)\n",
    "        \n",
    "        mask = path_line[i, 0] > 0\n",
    "        \n",
    "        target[i, mask] = class_label\n",
    "        \n",
    "    plt.imshow(target[l].cpu().numpy())\n",
    "    plt.title(f\"Class: {class_label}, Exit: ({exit_x}, {exit_y})\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "        \n",
    "        \n",
    "    \n",
    "    output = model(satellite)\n",
    "    L_ce = lb(output, target)\n",
    "    loss = L_ce\n",
    "    \n",
    "    print(f\"{output.shape}, {output.max()}, {output.min()}\")\n",
    "    print(f\"{target.shape}, {target.max()}, {target.min()}\")\n",
    "    \n",
    "    # optimizer.zero_grad()\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    \n",
    "    p = torch.argmax(output, dim=1)\n",
    "    plt.imshow(p[l].detach().cpu().numpy())\n",
    "    plt.title(f\"Predicted\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    valid_mask = target != 0\n",
    "    plt.imshow(valid_mask[l].detach().cpu().numpy())\n",
    "    plt.title(f\"Valid mask\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    break\n",
    "    \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "    \n",
    "#torch.save(model.state_dict(), \"model_200e_ce_new_dataset_3class.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_200e_ce_new_dataset_4class.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Plotting the loss graphs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, n_epochs + 1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, n_epochs + 1), test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Evaluation Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the accuracy graphs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, n_epochs + 1), train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(range(1, n_epochs + 1), test_accuracies, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Evaluation Accuracy Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "rs = [r for r in random.sample(range(len(dataset_test)), 6)]\n",
    "print(rs)\n",
    "\n",
    "cols = 6\n",
    "rows = 2\n",
    "fig = plt.figure(figsize=(24, 8))\n",
    "i = 1\n",
    "c = 0\n",
    "\n",
    "while i <= rows * cols:\n",
    "    r = random.randint(0, len(dataset_test))\n",
    "    sat = dataset_test[rs[c]][\"satellite\"].permute(1, 2, 0)\n",
    "    fig.add_subplot(rows, cols, i)\n",
    "    plt.imshow(sat)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    fig.add_subplot(rows, cols, i)\n",
    "    \n",
    "    s = dataset_test[rs[c]][\"satellite\"].to(device)\n",
    "    #print(s.shape)\n",
    "    s = s.unsqueeze(0)\n",
    "    output = model(s)\n",
    "    #output = torch.argmax(output, dim=1)\n",
    "    \n",
    "    output = output[0][0].detach().cpu().numpy()\n",
    "    \n",
    "    plt.imshow(output, cmap=\"inferno\")\n",
    "    plt.colorbar()\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    i += 1\n",
    "    c += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "#fig.savefig(f\"ckpt/{ck}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all three paths in one with proper labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "dataset_dir = \"../../dataset/dataset/train\"\n",
    "img_transform = ToTensor()\n",
    "path_transform = ToTensor()\n",
    "dataset_train = IntersectionDatasetClasses(root_dir=dataset_dir, \n",
    "                                    transform=img_transform,\n",
    "                                    path_transform=path_transform)\n",
    "\n",
    "dataset_dir = \"../../dataset/dataset/test\"\n",
    "img_transform = ToTensor()\n",
    "path_transform = ToTensor()\n",
    "dataset_test = IntersectionDatasetClasses(root_dir=dataset_dir,\n",
    "                                   transform=img_transform,\n",
    "                                   path_transform=path_transform)\n",
    "print(len(dataset_train))\n",
    "print(len(dataset_test))\n",
    "\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "b = 4\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=b, shuffle=True, num_workers=num_workers, pin_memory=True, persistent_workers=True, collate_fn=custom_collate_fn)\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=b, shuffle=True, num_workers=num_workers, pin_memory=True, persistent_workers=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) and m.kernel_size == (1, 1):\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.001)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    del(model)\n",
    "except NameError:\n",
    "    pass\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = u.UNet(n_channels=3, n_classes=5).to(device) # background, left, right, ahead, stacked\n",
    "model.apply(init_weights)\n",
    "\n",
    "try:\n",
    "    del(optimizer)\n",
    "except NameError:\n",
    "    pass\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "try:\n",
    "    del(scheduler)\n",
    "except NameError:\n",
    "    pass\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=1, eta_min=1e-5)\n",
    "\n",
    "try:\n",
    "    del(scaler)\n",
    "except NameError:\n",
    "    pass\n",
    "scaler = GradScaler(device=device)\n",
    "\n",
    "class_counts = torch.tensor([152000, 2000, 2000, 2000, 2000], dtype=torch.float)\n",
    "weights = 1.0 / class_counts\n",
    "weights = weights / weights.sum()\n",
    "lb = torch.nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "lb = lb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    satellite = batch[\"satellite\"].to(device)\n",
    "    path_line = batch[\"paths\"]\n",
    "    \n",
    "    plt.imshow(satellite[0].permute(1, 2, 0).cpu().numpy())\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(path_line[0][0][\"path_line\"].shape)\n",
    "    l = len(path_line[0])\n",
    "    \n",
    "    B = len(path_line)  # Number of batches\n",
    "    _, H, W = path_line[0][0][\"path_line\"].shape  # Height and width of the path_line\n",
    "    combined = torch.full((B, H, W), 0, dtype=torch.long)\n",
    "    \n",
    "    for i in range(l):\n",
    "        p = path_line[0][i][\"path_line\"]\n",
    "        ee = path_line[0][i][\"ee_data\"]\n",
    "        exit_x = ee[\"exit\"][\"x\"]\n",
    "        exit_y = ee[\"exit\"][\"y\"]\n",
    "        \n",
    "        class_label = map_exit_to_class(exit_x, exit_y)\n",
    "        \n",
    "        mask = p[0] > 0\n",
    "        \n",
    "        class_label = torch.full((B, H, W), class_label, dtype=torch.long)\n",
    "        \n",
    "        combined += mask * class_label\n",
    "    combined = combined.clamp(0, 4).to(device)\n",
    "        \n",
    "        # combined += path_line[0][i][\"path_line\"][0] * class_label\n",
    "        # combined = combined.clamp(0, 3)\n",
    "        \n",
    "    \n",
    "    plt.imshow(combined[0].cpu().numpy(), cmap=\"inferno\")\n",
    "    plt.colorbar()\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(\"Passing through the model\")\n",
    "output = model(satellite)\n",
    "print(\"Output shape:\", output.shape)  # Expected: torch.Size([1, 1, 400, 400])\n",
    "L_ce = lb(output, combined)\n",
    "print(\"Loss calculation\")\n",
    "loss = L_ce\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "output[0].shape\n",
    "\n",
    "plt.imshow(output[0][0].detach().cpu().numpy(), cmap=\"inferno\")\n",
    "plt.colorbar()\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "n_epochs = 500\n",
    "alpha = 0.5\n",
    "epochs = tqdm(range(n_epochs), desc=\"Training\", unit=\" epoch\")\n",
    "\n",
    "\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "for epoch in epochs:\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    running_train_correct = 0\n",
    "    running_train_total = 0\n",
    "    \n",
    "    batches = tqdm(train_dataloader, desc=\"Batches\", unit=\" batch\", leave=False)\n",
    "    \n",
    "    for batch in batches:\n",
    "        satellite = batch[\"satellite\"].to(device, non_blocking=True)\n",
    "        class_labels = batch[\"class_labels\"].to(device, non_blocking=True)\n",
    "        #path_line = batch[\"paths\"]\n",
    "        \n",
    "        class_labels = class_labels.squeeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(\"cuda\"):\n",
    "            output = model(satellite)\n",
    "            loss = lb(output, class_labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_train_loss += loss.item()\n",
    "        \n",
    "        p = torch.argmax(output, dim=1)\n",
    "        running_train_correct += (p == class_labels).sum().item()\n",
    "        running_train_total += class_labels.size(0)\n",
    "        \n",
    "        batches.set_postfix({\"Loss\": loss.item()})\n",
    "        \n",
    "        \n",
    "    avg_train_loss = running_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracy = running_train_correct / running_train_total\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    running_test_correct = 0\n",
    "    running_test_total = 0\n",
    "    \n",
    "    test_batches = tqdm(test_dataloader, desc=\"Batches\", unit=\" batch\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in test_batches:\n",
    "            satellite = batch[\"satellite\"].to(device, non_blocking=True)\n",
    "            class_labels = batch[\"class_labels\"].to(device, non_blocking=True)\n",
    "            #path_line = batch[\"paths\"]\n",
    "            \n",
    "            class_labels = class_labels.squeeze(1)\n",
    "            \n",
    "            with autocast(\"cuda\"):\n",
    "                output = model(satellite)\n",
    "                loss = lb(output, class_labels)\n",
    "            \n",
    "            running_test_loss += loss.item()\n",
    "            \n",
    "            p = torch.argmax(output, dim=1)\n",
    "            running_test_correct += (p == class_labels).sum().item()\n",
    "            running_test_total += class_labels.size(0)\n",
    "            \n",
    "            test_batches.set_postfix({\"Loss\": loss.item()})\n",
    "            \n",
    "    test_batches.close()\n",
    "            \n",
    "    avg_test_loss = running_test_loss / len(test_dataloader)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    \n",
    "    test_accuracy = running_test_correct / running_test_total\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    scheduler.step()  \n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'test_losses': test_losses,\n",
    "            'train_accuracies': train_accuracies,\n",
    "            'test_accuracies': test_accuracies,\n",
    "        }\n",
    "        os.makedirs('./ckpt', exist_ok=True)\n",
    "        torch.save(checkpoint, f'./ckpt/bce_checkpoint_epoch_{epoch + 1}_5classes.pth')\n",
    "        \n",
    "    epochs.set_postfix({\"Train Loss\": avg_train_loss, \"Test Loss\": avg_test_loss, \"Train Accuracy\": train_accuracy, \"Test Accuracy\": test_accuracy})\n",
    "    batches.close()\n",
    "    \n",
    "epochs.close()\n",
    "\n",
    "    \n",
    "#torch.save(model.state_dict(), \"model_200e_ce_new_dataset_3class.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Passing through the model\")\n",
    "output = model(satellite)\n",
    "print(\"Output shape:\", output.shape)  # Expected: torch.Size([1, 1, 400, 400])\n",
    "L_ce = lb(output, combined)\n",
    "print(\"Loss calculation\")\n",
    "loss = L_ce\n",
    "loss.item()\n",
    "\n",
    "\n",
    "plt.imshow(output[0][0].detach().cpu().numpy(), cmap=\"inferno\")\n",
    "plt.colorbar()\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".msc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
