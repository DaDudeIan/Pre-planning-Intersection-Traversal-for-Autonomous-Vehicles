#import "../../lib/mod.typ": *
 
= Results   <c5:results>

This section details the experiments conducted on the trained models created during this thesis project. Each section will focus on a specific loss method, detailed in @tab:loss-combinations. Within each of these sections, both qualitative and quantitative results will be presented. The qualitative results will be illustrated with images generated by the models. The models' weights will be instantiated with the saved weights from different training epochs, picked from the sets $S_"epochs"$ in @tab:loss-combinations. The quantitative results will be in the form of training and validation graphs, showing the evolution of the loss and accuracy as the models' training progressed, and tables showing the mIoU achieved by the models. This approach to showing the results is chosen to provide a clear and concise overview of the models' performance, both in terms of their ability to learn the task and their generalization capabilities, through glimpses into the results generated at varying but comparative stages of training.

The images used for the qualitative results were chosen from the validation set. The two chosen images present different scenes and thus offer different challenges to the models. The first image is of a simple intersection, with two roads intersecting perpendicularly, but with turn lanes and slightly curvy roads. The second image is of an intersection where the angle between the two roads is not perpendicular and there are a fair amount of clear road markings. The images are shown in @fig:res_comp along with their ground truth labels.

#let fig1 = image("../../figures/img/results/comp/satellite_test1.png")
#let fig2 = image("../../figures/img/results/comp/class_labels_test1.png")
#let fig3 = image("../../figures/img/results/comp/satellite_test2.png")
#let fig4 = image("../../figures/img/results/comp/class_labels_test2.png")

#std-block(breakable: false)[
  #figure(
    grid(
      columns: (1fr, 1fr, 1fr, 1fr),
      column-gutter: 0mm,
      fig1, fig2, fig3, fig4,
      subfigure("(a)"), subfigure("(b)"), subfigure("(c)"), subfigure("(d)")
    ),
    caption: [Two test images from the validation dataset along with their ground truth labels.]
  ) <fig:res_comp>
]

#h(1em) The following sections will each contain the same set of images with the predicted paths on top, as to give a clearer view for easier interpretation of what the models output. Each figure in the following sections will be presented as follows. Each figure consists of two rows and four columns. The top row of each figure is the first test image, @fig:res_comp#subfigure("a"), with the predicted paths shown on top of it. The second row of each figure is the second test image, @fig:res_comp#subfigure("c"), also with the predicted paths shown on top of it. Each column is from the outputs of each of the four models, DeepLabV3+, U-Net, ViT, and Swin, respectively.

As will be evident from the training and validation graphs, the models trained shows clears signs of overfitting. Therefore, a result of giving the models an image from the training dataset will be shown as well. This is to illustrate the models' ability to eventually learn the task, given large enough datasets and training time.

Finally, the mean Intersection over Union (mIoU) will be shown for the models trained with the different loss methods. The mIoU is a common metric used in semantic segmentation tasks, and it provides a measure of how well the model's predictions align with the ground truth labels. The mIoU is calculated by taking the intersection of the predicted and true labels for each class, dividing it by the union of the predicted and true labels, and then averaging this value across all classes. Thus, a higher value is better, with a value of 1 being perfect. Furthermore, the IoU for each class will be shown as well. Each value in the tables will be the average of the mIoU and IoU values from the validation set. To ensure clarity, the class labels are as follows: 0 = background #ball(black), 1 = left-hand turn #ball(color.rgb("#550f6b")), 2 = right-hand turn #ball(color.rgb("#bb3754")), 3 = straight ahead #ball(color.rgb("#f38b08")), and 4 = layered #ball(color.rgb("#fcfea4")). Each section will contain their own table, but a final section will present of all the models' mIoU values, to allow for a clear comparison between the different loss methods. 

As the predicted outputs are of a high resolution, the mIoU values are expected to be low, except for the background class, which is expected to be high due to its high presence. As seen in @fig:res_comp#subfigure("b") and @fig:res_comp#subfigure("d"), the ground truth paths are thin and therefore difficult to predict. This may especially be the case when the models are fully trained and may have generalized in such a fashion that they may choose slightly different paths than those in the dataset. Therefore, the per-class IoU and mIoU are taken across the entire validation set as to ensure that the values are neither unexpectedly high nor low, but generate a realistic view of the models' performance.

All experiments were conducted on the same hardware. The desktop features an Intel Core i7-9700K CPU, 16GB DIMM DDR5 RAM, and an NVIDIA GeForce RTX 2070 GPU with 8GB VRAM. All data was stored on an NVMe M.2 SSD, with the model checkpoints being saved to a SATA HDD. The environment on the desktop is Windows 11 Pro, utilizing the Windows Subsystem for Linux (WSL) running the Linux distribution Ubuntu 24.04. Although the computational capabilities of the machine should not influence the results for this specific experimental setup if replicated, it should be noted that larger systems allow for larger and more complex models, as well as larger batches of data to be processed at once, likely achieving better results.

#include "ce.typ"

#include "ce_cont.typ"

#include "cmap.typ"

#include "ce_cmap.typ"

#include "miou.typ"