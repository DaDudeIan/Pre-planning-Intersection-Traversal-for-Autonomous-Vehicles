#import "../../../lib/mod.typ": *

== Loss Function Design <c4:loss>

// Bare bones loss function: dot product
// 
// class imbalance (a lot more background pixels than path pixels)
// - Binary Cross Entropy (BCE). Binary segmentation, pixel-wise classification.
// - Dice loss. Overlap between the predicted and ground truth masks. (https://paperswithcode.com/paper/generalised-dice-overlap-as-a-deep-learning)
// - IoU loss. Intersection over Union. Differential approximation (soft IoU loss)
// - Focal loss.
// 
// Distance based
// - Hausdorff distance. Maximum distance between two sets. Penalizes outliers.
// - Chamfer distance. Average distance between two sets.
// 
// Smoothness
// - Heavily penalize breaks. Topology analysis.
// - Total Variation (TV) loss. Penalize sharp changes.
// 
// Dot product
// + BCE or Dice
// + Distance based (Hausdorff, Chamfer)
// + Topology/Smoothness
// 
// For diffusion: combine with standard diffusion loss (MSE)

One of the most critical parts of designing a deep learning model, is the creation of the loss function that will guide the training. The loss function is a measure of how well the model is performing, and it is used to adjust the model's parameters during training. Therefore, the choice of loss function is crucial to the success of the model. In this section, I will discuss the design of the loss functions used to train the models I've set out to create. It will consist of a combination of different loss functions, each designed to capture different aspects of the problem at hand. Firstly, I will cover the development of the novel "cold map"-based part of the loss function, which is supposed to guide the model by penalizing points further away from the true path subject to some threshold. Secondly, I will discuss the use of a commonly used loss function, the #acr("BCE") loss, which is used for binary segmentation tasks. Finally, the last part of the loss function will deal with the actual topology of the predicted path. It will heavily penalize breaks in the path, branches in paths, and other topological errors, such as not reaching connecting the entry and exit. With these three different loss functions, the goal is to compare different combinations of them to determine which one works best for the task at hand. 

=== Cold Map Loss <c4:cold_loss>

The first part of the loss function is the cold map loss. This involves using the predicted path generated by the model, and comparing it to the cold map from the dataset. The creation of the cold maps are detailed in @c4:cold_maps. Briefly, the cold maps are grids of the same size as the input image, where the intensity of each cell is a value derived from the distance to the nearest path pixel magnified beyond some threshold. 

The main idea behind the cold map loss is to introduce spatial penalty that increases as the distance from the true path increases. Though it is similar to #acr("BCE"), it differs in some key aspects. It is not pixel-wise, but rather a global loss that is calculated over the entire image. This means that slight deviations from the true path are penalized less than those with a larger discrepancy. This property of the loss function is a desirable trait for path-planning tasks, as minor offsets from the true path are less critical than larger ones. This loss function is defined as follows:

$
  cal("L") = sum_(i=1)^H sum_(j=1)^W C_(i j) P_(i j)
$ <eq:loss_cold>

where $C_(i j)$ is the cold map value at pixel $(i, j)$, and $P_(i j)$ is the predicted path value at pixel $(i, j)$. This version of the loss function is a simple dot product between the cold map and the predicted path. Thus, after flattening the cold map and the predicted path matrices, the loss is calculated as the dot product between the two vectors: 

$
  cal("L")_"cold" = C dot P
$ <eq:loss_cold_flat>

where $C$ is the cold map vector and $P$ is the predicted path vector, giving a scalar value, contributing to the total loss. This value does, however, grow extremely quickly, as the dot product, as shown by @eq:loss_cold, is simply a sum over the entire image. This means that the loss value will be very high, even for small deviations from the true path, and extremely high for large deviations or noisy images, as expected from the model during early stages of training. While this rapid growth can be combated by introducing a very low weight to the loss function, doing so would also mean that smaller deviations become irrelevant, which in undesired. Thus, inspired by BCE, I will introduce a mean reduction to the loss function, which will divide the loss by the number of pixels in the image. This will ensure that the loss value is more stable and that the model can learn from smaller deviations. With this, the implementation of a cold map loss, will be based on the following equation:

$
  cal("L")_"cold" = 1 / (H dot W) sum_(i=1)^H sum_(j=1)^W C_(i j) P_(i j) 
$ <eq:loss_cold_mean>

where $H$ and $W$ are the height and width of the image, respectively. The implementation of @eq:loss_cold_mean is very straightforward, and is shown in the code listing below:

#listing([
  ```python
def cmap_loss(cmap_gt, path_pred, reduction) -> torch.Tensor:
  cmap_f = torch.flatten(cmap_gt)
  path_f = torch.flatten(path_pred)
  
  loss = torch.dot(cmap_f, path_f)
    
  return loss if reduction != 'mean' else loss / len(cmap_f)

class CmapLoss(nn.Module):
  def __init__(self, weight: float = 1.0, reduction: str = 'mean'):
    super(CmapLoss, self).__init__()
    self.weight = weight
      
  def forward(self, cmap_gt: torch.Tensor, path_pred: torch.Tensor):
    loss = cmap_loss_torch(cmap_gt, path_pred, self.reduction)
    return self.weight * loss
  ```
],
caption: [Implementation of the cold map loss calculation using PyTorch.]
) <code.cold_loss>

To implement this loss function, a class `CmapLoss` is created, which inherits from `torch.nn.Module`. The class has a single parameter, `weight`, which is used to scale the loss value. The `forward` method takes the ground truth cold map `cmap_gt` and the predicted path `path_pred` as inputs. The method then calculates the loss using the `cmap_loss` function and scales it by the `weight` parameter. The function `cmap_loss` takes two PyTorch tensors as inputs: `cmap_gt`, which represents the ground truth cold map, and `path_pred`, which is the predicted path output from the model. The cold map and predicted path are initially matrices with dimensions corresponding to the image's height and width. To compute the loss as a single scalar, both matrices are flattened into one-dimensional vectors. The function then calculates the dot product between these two vectors using `torch.dot`, effectively summing the element-wise products. If the `reduction` parameter is set to `mean`, the loss is divided by the number of elements in the vectors, which is the total number of pixels in the image. 

Examples of this loss function in action is shown in @fig:cmap_loss_comp. The left and center top row plots overlays a complete, single width path on top of the cold map. This highlights the fact that paths that are close to the true path, are penalized less and the further away, the more the penalty explodes in value. The last image in the top row shows how the loss handles a noisy image. As it can be seen, the loss is significantly higher than the rest. This is a desired trait of the loss function, as noise is just about the exact opposite of a continuous path. The bottom row shows alternate paths. The path on the rightmost image is the true path, which shows that if the path is dead on, then the penalty is none. The leftmost image, shows a path that is not connected. As shown, it still scores a perfect score. This is because the cold map loss only penalizes the distance from the path, not the topology of the path itself, so breaks will only result in a higher score. Lastly, the center image shows the true path, but with several branches. As seen in the loss value, this also incurs very little penalty. The bottom row of images highlight a dire need for a topology-based loss, which will be explored in @c4:topology_loss. 

#std-block(breakable: false)[
  #v(-1em)
  #box(
    fill: theme.sapphire,
    outset: 0em,
    inset: 0em,
  )
  #figure(
  image("../../../figures/img/loss_example/cmap_loss_comparison7.png", width: 100%),
  caption: [Paths drawn on top of a cold map with their associated loss above calculated using the function from @code.cold_loss. The top row shows fully connected paths, while the bottom row shows paths with breaks and branches, as well as the true path.]
) <fig:cmap_loss_comp>
]

=== Binary Cross-Entropy Loss <c4:bce_loss>

The #acr("BCE") loss is a commonly used loss function for binary segmentation tasks, which is relevant for the task at hand due to the pixel-subset nature of the problem, i.e. pixels can be either 1 or 0. Furthermore, it is well-versed in handling heavily imbalanced data, which is the case when dealing with classification tasks where one class is much more prevalent than the other, like path and non-path pixels in an image. These properties make it ideal for this problem, as the background pixels are much more prevalent than the path pixels. The implementation is using the definition from PyTorch #footnote([Full implementation details can be found in the official documentation: https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html]), which is defined as follows:


$
  cal("l")(x,y) = L = {l_1, dots , l_N}^T
$ <eq:bce_loss>
with
$
  l_n = - w_n [y_n log x_n + (1 - y_n) log(1 - x_n)]
$ <eq:bce_loss_n>
where $w_n$ is a weight parameter, $y_n$ is the ground truth label, $x_n$ is the predicted label, and $cal("l")$ is subject to
$
  cal("l")(x,y) = cases(
    #align(right)[#box[$"mean"(L)$], #h(5mm)]& "if reduction = 'mean'",
    #align(right)[#box[$"sum"(L)$],]& "if reduction = 'sum'"
  )
$ <eq:bce_loss_reduction>
depending on the reduction parameter. The left-hand side of @eq:bce_loss_n is activated when the ground truth label is $1$. It evaluates how well the positive class's predicted probability $x_n$ aligns with the ground truth. A smaller loss is achieved when the predicted probability is close to $1$. The right-hand side of @eq:bce_loss_n is activated when the ground truth label is $0$. It evaluates how well the negative class's predicted probability $x_n$ aligns with the ground truth. A smaller loss is achieved when the predicted probability is close to $0$. The BCE loss is then calculated as the sum or mean of the individual losses, depending on the `reduction` parameter. The weight parameter $w_n$ can be used to scale the output of the loss function. #acr("BCE") quantifies the dissimilarity between the predicted probabilities and the actual labels, giving a sense of how well the model is performing. For example, calculating the BCE with $y=1$ and $x=0.8$ gives

#let nonum(eq) = math.equation(block: true, numbering: none, eq)
#nonum($-1 dot (1 dot log(0.8) + (1 - 1) dot log(1 - 0.8)) = -log(0.8) = 0.223$)

This value represents the dissimilarity between the predicted probability of $0.8$ and the actual label of $1$. A lower value indicates that the model's prediction is closer to the ground truth, suggesting a more accurate classification. Alternatively, the value can be near $1$, indicating a large discrepancy between the predicted and actual labels. So, a value of $0.223$ is a good result, as it indicates that the model is performing well, with some room for improvement. For contrast, if the predicted label is $0.4$, but the true label is $1$, the dissimilarity would be $-log(0.4) approx 0.916$. This higher value reflects a more significant error in prediction. From this, note that the function calculates the dissimilarity for both positive and negative classes. So, in the case of the predicted label being $0.4$ and the true label being $0$, the loss value would be much better at just $-log(1-0.4) approx 0.51$. 

In the PyTorch implementation, the BCE loss can be either summed or averaged over the batch, depending on the `reduction` parameter. While using the sum method can provide stronger signals on rare but critical pixels, averaging might help maintain stability across batches, especially when dealing with very imbalanced datasets. Thus, the mean method is chosen for this project, as it leads to a more stable training process with smaller fluctuations in the loss values. The BCE loss is calculated using the following code snippet:

#listing([
  ```python
def bce_loss_torch(path_gt, path_pred, reduction) -> torch.Tensor:
    bce = torch.nn.BCELoss(reduction=reduction)

    bce_loss = bce(path_pred, path_gt)
    
    return bce_loss

class BCELoss(nn.Module):
  def __init__(self, weight: float = 1.0):
    super(BCELoss, self).__init__()
    self.weight = weight
      
  def forward(self, path_gt: torch.Tensor, path_pred: torch.Tensor, reduction: str = 'mean'):
    loss = bce_loss_torch(path_gt, path_pred, reduction)
    return self.weight * loss
  ```
],
caption: [Implementation of the #acr("BCE") loss function using PyTorch.]
) <code.bce_loss>

To implement the BCE loss function, a smaller wrapper class for the existing PyTorch implementation is created. The class `BCELoss` inherits from `torch.nn.Module` and has a single parameter, `weight`, which is used to scale the loss value. The `forward` method takes the ground truth path `path_gt`, the predicted path `path_pred`, and the reduction method as inputs. The method then calculates the loss using the `bce_loss_torch` function and scales it by the `weight` parameter. The function `bce_loss` calculates the BCE loss between the ground truth and predicted paths. It takes the paths to the ground truth and predicted images as input, reads them using OpenCV, and converts them to PyTorch tensors after normalization, since the images are stored as 8-bit greyscale images. The function creates a BCE loss `criterion` using `torch.nn.BCELoss` and calculates the loss using the ground truth and predicted tensors. The loss value is then returned as a float. In the tensor version of the function, the paths are already tensors, and the function can be called directly with the tensors as input.

#std-block(breakable: false)[
  #v(-1em)
  #box(
    fill: theme.sapphire,
    outset: 0em,
    inset: 0em,
  )
  #figure(
  image("../../../figures/img/loss_example/cmap_loss_comparison4.png", width: 100%),
  caption: [The ground truth #ball("#E4B96D") compared to some drawn path #ball(white). The losses above the plots are the BCE loss using function in @code.bce_loss using `mean` as the string passed as the reduction method. ]
) <fig:bce_loss_comp>
]

Examples of the `mean`-based BCE loss is shown in @fig:bce_loss_comp, showing various interesting aspects of the loss function. (b) shows an expectedly high loss value, as the path is far from the true path. This matches the case for the cold map loss. (a), (c), and (d) show very similar loss values, despite being vastly different, both in terms of closeness to the path, but also where they are going to and from, further highlighting the need for a topology-based loss. Even their sum counterparts show very similar values. Interestingly, the losses seen in (f) and (h) are very different, when the cold map based loss showed them as being equal. This shows that BCE is more sensitive to the topology of the path, but as (g) shows, it still gives a very low loss value when a path has branches. All of this shows that some topology analysis is needed for the loss function to capture realistic paths.

Other considerations for handling imbalanced data include methods like Dice @dice_loss similarity coefficient and Focal loss @focal_loss, which can also be effective in certain contexts. The Dice similarity coefficient is a measure of overlap between two samples, and is particularly useful when dealing with imbalanced data. The Focal loss is designed to address the class imbalance problem by focusing on hard examples that are misclassified. These methods can be used in conjunction with the BCE loss to improve the model's performance, especially when dealing with heavily imbalanced datasets. But for the purposes of this project, the BCE loss is expected to be sufficient to handle the class imbalance.




=== Topology Loss <c4:topology_loss>

// @topo1 also highlights the use of BCE in combination with a topology loss.
// Mention TopoLoss from TopoNets, but that the smoothing is not what is needed here.
// 

As the previous sections have highlighted, there is a dire need for a topology-based loss function. The cold map loss and the BCE loss are both excellent at penalizing paths that are far from the true path, but they do not penalize breaks in the path, branches in the path, or paths are only loosely driven towards being connected. This is where the topology loss comes in. The constituent parts of the topology loss are detailed in the following sections. First, the continuity part of the topology loss is discussed, which is designed to ensure that the predicted path is continuous and does not contain any breaks. This is done by aiming for specific Betti number values. Second, eliminating branches in the path is discussed, which is crucial for ensuring that the predicted path is a single connected component. Finally, the entry and exit part of the topology loss is discussed, which is designed to ensure that the predicted path connects the entry and exit points of the intersection.

Considerations of using existing topology existing methods. Dep #etal @topoloss introduced TopoNets and TopoLoss. This loss function revolves around penalizing jagged paths and encouraging smooth, brain-like topographic organization within neural networks by reshaping weight matrices into two-dimensional cortical sheets and maximizing the cosine similarity between these sheets and their blurred versions. Cortical sheets are two-dimensional grids formed by reshaping neural network weight matrices to emulate the brain's spatial organization of neurons, enabling topographic processing. While initially interesting in the context of this project, simple testing showed that the values returned from this loss, did not give a proper presentation of the path's topology, outside of its smoothness. And while smoothness is a part of the topology, this is will largely be handled by the #acr("BCE") loss.

// ==== Continuity <c4:topo_cont>

// The first part of the topology loss is the continuity part. This part of the loss function is crucial for ensuring that the predicted path is continuous and does not contain any breaks. Breaks in a path would be unrealistic for a grounded vehicle to follow. To understand the continuity part of the topology loss, it is essential to understand the concept of Betti numbers as described below:

// #std-block(breakable: true)[
//   #box(
//     fill: theme.sapphire.lighten(10%),
//     outset: 1mm,
//     inset: 0em,
//     radius: 3pt,
//   )[#text(white, size: 12pt, font: "JetBrainsMono NFM")[Betti Numbers]] \
//   Betti numbers @betti come from algebraic topology, and are used to distinguish topological spaces based on the connectivity of $n$-dimensional simplicial complexes. The $n$th Betti number, $beta_n$, counts the number of $n$-dimensional holes in a topological space. The Betti numbers for the first three dimensions are:
//   - $beta_0$: The number of connected components.
//   - $beta_1$: The number of loops.
//   - $beta_2$: The number of voids.
//   The logic follows that, in 1D, counting loops are not possible, as it is simply a line. This, if the number is greater than 1, it means it is split into more than component. In 2D, the number of loops is counted, i.e. a circled number of pixels. In 3D, this extends to voids. 
// ]

// With this, for the 2D images used in this project, the Betti numbers are $beta_0$ and $beta_1$. The continuity part of the topology loss is designed to ensure that the predicted path has a single connected component and no loops. This is achieved by aiming for the Betti numbers to be $beta_0 = 1$ and $beta_1 = 0$. Higher dimensional Betti numbers are not relevant for this project, as the images are 2D. While Betti numbers are a powerful tool for topology analysis, they are not directly applicable to the loss function as they are discrete values. This means that offer no gradient information, which is essential for training a neural network. Instead, persistent homology is deployed.

// Persistent homology is a mathematical tool used to study topological features of data. Homology itself is a branch of algebraic topology concerned with procedures to compute the topological features of objects. Persistent homology extends the basic idea of homology by considering not just a single snapshot of a topological space but a whole family of spaces built at different scales. Instead of calculating Betti numbers for one fixed space a filtration is performed. This filtration is a sequence of spaces, where each space is a subset of the next, i.e. a nested sequence of spaces where each one is built by gradually growing the features by some threshold. As this threshold varies, topological features such as connected components and loops will appear (be born) and eventually merge or vanish (die).

// This birth and death of features is recorded in what is known as a persistence diagram or barcode (See @fig:persistent_homology). In these diagrams, each feature is represented by a bar (or a point in the diagram) whose length indicates how persistent, or significant, the feature is across different scales. Features with longer lifespans are generally considered to be more robust and representative of the underlying structure of the data, whereas those that quickly appear and disappear might be attributed to noise.
// // possibility of exploding loss when noisy image


// #let fig1 = { image("../../../figures/img/loss_example/comp12.png", width: 80%) }
// #let fig2 = { image("../../../figures/img/loss_example/pers_bar.png") }
// #let fig3 = { image("../../../figures/img/loss_example/pers_diag.png") }

// #let fig4 = { image("../../../figures/img/loss_example/comp7.png", width: 80%) }
// #let fig5 = { image("../../../figures/img/loss_example/pers_bar_2.png") }
// #let fig6 = { image("../../../figures/img/loss_example/pers_diag_2.png") }

// #std-block(breakable: false)[
//   #v(-1em)
//   #box(
//     fill: theme.sapphire,
//     outset: 0em,
//     inset: 0em,
//   )
//   #figure(
//   grid(
//   columns: (1fr, 1fr, 1fr),
//   column-gutter: 1mm,
//   align: (center, center),
//   fig1, fig2, fig3,
//   //fig4, fig5, fig6
  
// ),
// caption: [The top row shows a connected path along with its persistence barcode and persistence diagram, while the bottom row shows a disconnected path. The number of lines in the barcode, stems from the fact that the images are rather large in size and thus the number of built spaces are many.]
// ) <fig:persistent_homology>
// ]

// Since sources are very scarce regarding the implementation of the differentiable wasserstein distance, an approximation is made for the backwards pass as shown in @code:persistent_homology. The entire implementation is shown in the code listing below: 

// #listing([
//   ```python
// class PersistentHomologyLossFunction(torch.autograd.Function):
//   def forward(ctx, input_tensor, target_betti, threshold):
//     array = input_tensor.detach().cpu().numpy()
//     array_inv = 1.0 - array
//     cc = gd.CubicalComplex(top_dimensional_cells=array_inv)
    
//     computed_betti = {}
//     for dim, (birth, death) in cc.persistence():
//       if death == float('inf') or death > threshold:
//         computed_betti.setdefault(dim, 0)
//         computed_betti[dim] += 1

//     ctx.computed_betti = computed_betti
//     ctx.target_betti = target_betti
//     ctx.input_shape = input_tensor.shape

//     loss_sq = 0.0
//     for dim, target in target_betti.items():
//       comp = computed_betti.get(dim, 0)
//       diff = comp - target
//       loss_sq += diff * diff
//     loss_value = np.sqrt(loss_sq)

//     ctx.loss_value = loss_value
//     ctx.diff_dict = {dim: computed_betti.get(dim, 0) - target for dim, target in target_betti.items()}

//     return torch.tensor(loss_value, dtype=input_tensor.dtype, device=input_tensor.device)

//   def backward(ctx, grad_output):
//     total_diff = 0.0
//     for diff in ctx.diff_dict.values():
//       total_diff += diff

//     L_val = ctx.loss_value if ctx.loss_value > 1e-8 else 1e-8
//     grad_scalar = total_diff / L_val
//     grad_input = grad_output * grad_scalar 
//                   * torch.ones(ctx.input_shape, device=grad_output.device)

//     return grad_input, None, None

// class PersistentHomologyLoss(nn.Module):
//   def __init__(self, target_betti, threshold=0.5):
//     super(PersistentHomologyLoss, self).__init__()
//     self.target_betti = target_betti
//     self.threshold = threshold

//   def forward(self, input_tensor):
//     return PersistentHomologyLossFunction.apply(input_tensor, 
//                                                 self.target_betti, 
//                                                 self.threshold)
//   ```
// ],
// caption: [Persistent Homology implementation.]
// ) <code:persistent_homology>

// The loss function consists of two classes, `PersistentHomologyLoss` is the wrapper class that inherits from `torch.nn.Module`. It takes two parameters, `target_betti` and `threshold`. The `target_betti` parameter is a dictionary containing the target Betti numbers for the input tensor, while the `threshold` parameter is the threshold value used in the persistent homology calculation. The `forward` method of the `PersistentHomologyLoss` class calls the `PersistentHomologyLossFunction` class, which is a custom autograd function that performs the persistent homology calculation. The `PersistentHomologyLossFunction` class inherits from `torch.autograd.Function` and has two methods, `forward` and `backward`. 

// The `forward` method takes the input tensor, target Betti numbers, and threshold as inputs. First, the tensor is converted to a numpy array so it can be used by the Gudhi function creating the cubical complex. The cubical complex is created using the `CubicalComplex` class, which constructs a cubical complex from the input tensor. The persistence of the cubical complex is then computed using the `persistence` method, which returns the birth and death values of the topological features. The computed Betti numbers for each dimension are then calculated and stored in the `computed_betti` dictionary. The loss value is calculated by comparing the computed Betti numbers to the target Betti numbers and summing the squared differences. The square root of the sum is then returned as the loss value:

// $
//   cal("L")_"cont" = sqrt(sum_"dim" (beta^"dim"_"computed" - beta^"dim"_"target")^2 )
// $

// The `backward` method calculates the gradient of the loss value with respect to the input tensor. The total difference between the computed and target Betti numbers is calculated, and the gradient is computed as the total difference divided by the loss value. The gradient is then multiplied by the output gradient and returned as the gradient. 

// Formally, there is no gradient to discrete values, so a surrogate gradient is required. First, the aggregate discrepancy is defined as 

// $
//   Delta = sum_(dim) (beta^"dim"_"computed" - beta^"dim"_"target")
// $

// The `backward` method approximates the derivative of the loss with respect to the input by simply computing a scalar gradient:

// $
//   g = Delta / L
// $

// where $L = max(epsilon, cal("L")_"cont")$ with $epsilon = 10^(-8)$. This scalar $g$ is then uniformly distributed over all elements of the input tensor. Finally, the gradient with respect to the input is given by

// $
//   gradient_x L = g dot "grad_output"
// $
// where $"grad_output"$ is the upstream gradient.

// Creating and using the `PersistentHomologyLoss` class is straightforward. The target Betti numbers are defined as a dictionary with the desired Betti numbers for each dimension. The threshold parameter is also defined in a variable and passed during initialization:

// #listing([
//   ```python
// target_betti = {0: 1, 1: 0}
// topo_loss_fn = PersistentHomologyLoss(target_betti, threshold=0.75)
// loss = topo_loss_fn(p_tensor)
//   ```
// ],
// caption: [Creating and using the `PersistentHomologyLoss` class.]
// ) <code.persistent_homology_usage>

// Examples of the persistent homology loss is shown in @fig:loss_topo_cont. Remember, the desired Betti numbers are $beta_0 = 1$ and $beta_1 = 0$, meaning that any more than 1 component and any loops should be penalized. As expected, the ground truth in (a) has a loss value of $0$. (b) showcases the fact that if parts of the true path are missing, the loss value will swiftly increase. (c) shows this even better, as it consists of 4 different component, thus achieving a difference of 3. (d) shows that the loss function also penalizes loops, as the loss value is $1$. 



// #std-block(breakable: false)[
//   #v(-1.2em)
//   #box(
//     fill: theme.sapphire,
//     outset: 0em,
//     inset: 0em,
//   )
//   #figure(
//   image("../../../figures/img/loss_example/topo_loss_plots_report2.png", width: 100%),
//   caption: [ Results of using the continuity part of the topology loss. ]
//   ) <fig:loss_topo_cont>
// ]


// ==== Branching <c4:topo_branch>

// The second part of the topology loss is the branching part. Branching refers to the presence of dead-ends in the predicted path. Dead-ends are points in the path that stop abruptly. These are undesired as a vehicle will only need one path to follow through an intersection and dead-end branches might stop in the middle of the intersection or somewhere completely irrelevant. This section of the loss function is another crucial part that goes along nicely with the continuity part because, while the continuity parts ensures that only one component is present, a branching path will still only total one component. So, the continuity part handles component count and loops, while the branching part handles undesired dead-ends. The only desired dead-ends are the entry and exit points of the intersection. How these are handled is discussed in @c4:topo_entry.

// This part of the loss function will focus on counting the number of endpoints of the predicted path. The goal is to penalize paths that have more than two endpoints, as this indicates that the path has branches. This will be achieved by using an altered kind of the 8-neighbour grid. In this grid, the value at each pixel is decided by how many neighbours is has as defined by some kernel. In the 8-neighbour grid, the kernel is defined as

// $
//   k = mat(
//     1, 1, 1;
//     1, 0, 1;
//     1, 1, 1;
//   )
// $ <branch_kernel>

// resulting in each grid point having a value stored equal to that of the number of occupied pixels surrounding it, naturally ignoring itself, after convolution. In the context of detecting branches, the desired value for all pixels is 2, other than the entry and exit points, which are allowed to have 1 as the only points. A value of 2 is allowed since this means that the pixel is on the path, i.e. each pixel has their own entry and exit neighbour points. This definition does, however, become troublesome when images are not binary, i.e. the path may be subject to some kind of smoothing or anti-aliasing. For example, if the pixels near the exit point have some value, they would be counted as neighbours, meaning that the exit point would not be considered an endpoint. Therefore, in the implementation in @code:branching_loss, a soft threshold is employed instead of a hard one.

// Also to be considered for this, is the fact that it should be differentiable. Thankfully, a soft threshold is differentiable, as it is simply a sigmoid function. The input tensor is put through the following equation:
// $
//   "input_tensor" = sigma(alpha ( "input_tensor"/255 - t))
// $
// where $t$ is the threshold, $alpha$ is the steepness of the sigmoid, and $sigma$ is the sigmoid function. This approach gives way for a differentiable approximation to a binary threshold, and a high value for $alpha$ will make the threshold very sharp, while a low value will make it very soft. Tests of various values for $alpha$ and $t$ were conducted early and can be found in @app:branch_loss_tests. From this, the values for chosen to be $alpha = 70.0$ and $t = 0.85$. This is also reflected in the code. These high values were chosen to ensure that the threshold is very sharp, and points that occur as a result of anti-aliasing are not counted as pixels in the actual end-points count.

// Next, the neighbour sum is calculated using the kernel defined in @branch_kernel. This is done by convolving the input tensor with the kernel using the `conv2d` function from PyTorch, resulting in a tensor where each pixel contains the number of occupied pixels in its 8-neighbourhood:
// $
//   "neighbour_sum"_(x,y) = sum_(i=-a)^a sum_(j=-b)^b k_(i,j) f_(x-i,y-j)
// $
// where $k$ is the kernel, $f$ is the input tensor, and $a$ and $b$ describe the size of the kernel by $-a <= i <= a$ and $-b <= j <= b$. In this case, $a = b = 1$ as the kernel is 3x3. `padding = 1` ensures equal stride.

// However, instead of directly counting the number of endpoints, the number of endpoints is calculated using a Gaussian function. This is done to ensure that the loss function is differentiable. The indicator function is defined as 
// $
//   "indicator" = exp(-(("neighbour_sum" - 1)^2) / (2 sigma^2))
// $ <branch_indicator>
// where $sigma$ is some defined value, here set to $sigma = 0.1$ as it then sharply peaks around 1. This further works as a smooth approximation compared to a strong threshold of $1$. Finally, the loss calculated by finding the number of endpoints with
// $
//   "endpoints" = "input_tensor" dot "indicator"
// $
// and squaring the difference between the number of endpoints and the target number of endpoints:
// $
//   cal("L")_"branch" = (E-T)^2
// $ <branch_dedt>
// where $E = sum "endpoints"$ and $T$ is the target number of endpoints. The loss is then returned as a tensor.



// #listing([
//   ```python
// class EndpointsLossFunction(torch.autograd.Function):
//   def forward(ctx, input_tensor, target_ee, alpha = 70.0, threshold = 0.85):
//     input_tensor = input_tensor / 255.0
//     input_tensor = torch.sigmoid(alpha * ((input_tensor) - threshold))

//     p = input_tensor.unsqueeze(0).unsqueeze(0)
    
//     kernel = torch.tensor([[1, 1, 1], 
//                            [1, 0, 1], 
//                            [1, 1, 1]], 
//                           dtype=torch.float32).unsqueeze(0).unsqueeze(0)
    
//     neighbour_sum = F.conv2d(p, kernel, padding=1).squeeze(0).squeeze(0)
    
//     sigma = 0.1
//     desired_endpoints = 1
//     indicator = torch.exp(-((neighbour_sum - desired_endpoints) ** 2) / (2 * sigma ** 2))
    
//     endpoints_map = input_tensor * indicator
//     measured_endpoints = torch.sum(endpoints_map)

//     loss_value = (measured_endpoints - target_ee) ** 2
    
//     ... save values to ctx ...
    
//     return loss_value

//   def backward(ctx, grad_output):

//     ... load ctx values ...

//     E = (input_tensor * indicator).sum()
    
//     dL_dE = 2 * (E - target_ee)

//     grad_direct = indicator

//     factor = -(neighbour_sum - 1) / (sigma ** 2)
    
//     conv_input = (input_tensor * indicator * factor).unsqueeze(0).unsqueeze(0)
//     grad_indirect = F.conv2d(conv_input, kernel, padding=1).squeeze(0).squeeze(0)
    
//     grad_total = grad_direct + grad_indirect
    
//     soft_thresholded = torch.sigmoid(alpha * (input_tensor - threshold))
//     sigmoid_deriv = (alpha / 255.0) * soft_thresholded * (1 - soft_thresholded)
    
//     grad_input = grad_output * dL_dE * grad_total * sigmoid_deriv
    
//     return grad_input, None
    
// class EndpointsLoss(nn.Module):
//   def __init__(self, target_ee, alpha = 70.0, threshold = 0.8):
//     super(EndpointsLoss, self).__init__()
//     self.target_ee = target_ee
//     self.alpha = alpha
//     self.threshold = threshold

//   def forward(self, input_tensor):
//     return EndpointsLossFunction.apply(input_tensor, self.target_ee, 
//                                        self.alpha, self.threshold)
//   ```
// ],
// caption: [Branching loss implementation.]
// ) <code:branching_loss>


// // During backpropagation, the loss gradient is computed with respect to the input. The total derivative considers two contributions: the direct contribution from the indicator function and the indirect contribution from the convolution of the input tensor with the kernel. First, the direct contribution is found by taking the derivative of @branch_indicator with respect to the pixel value $p$:
// // $
// //   partial/(partial p) p dot "indicator" = "indicator"
// // $

// // Since the pixel values also affect the indicator indirectly through the neighbour sum, the indirect contribution comes from differentiating the indicator with respect to the neighbour sum:
// // $
// //   (partial"indicator")/(partial"neighbour_sum") = -("neighbour_sum" - 1) / sigma^2 dot "indicator"
// // $
// // As seen in the implementation, the convolution of the product $p dot "indicator" dot (- ("neighbour_sum" - 1)/sigma^2)$. This is a result of the chain rule being applied to the derivative of the loss with respect to the input tensor. Another part of the total derivative is the simple loss found in @branch_dedt. The derivative of this simply becomes
// // $
// //   2 * (E - T)
// // $
// // where $E$ is the number of endpoints and $T$ is the target number of endpoints. Finally, continuing the chain rule through the sigmoid transformation by multiplying with the derivative of sigmoid function gives 
// // $
// //   "sigmoid_deriv" = alpha / 255 * "soft_thresholded" * (1 - "soft_thresholded")
// // $
// // Multiplying all these components together gives the gradient with respect to the input tensor.

// To explain the backpropagation of this part of the loss function, I will re-introduce and define the following quantities:


// #let b1 = std-block(breakable: false)[
//   #box(
//     fill: theme.sapphire.lighten(10%),
//     outset: 1mm,
//     inset: 0em,
//     radius: 3pt,
//   )[#text(white, size: 12pt, font: "JetBrainsMono NFM")[Soft-threshold]] \
//   $
//     p = sigma ( alpha ( x / 255 - t))
//   $ <soft_threshold>
//   where 
//   - $p$ is the soft-thresholded pixel value, 
//   - $x$ is the input, 
//   - $t$ is the threshold, 
//   - $sigma$ is the sigmoid function, 
//   - $alpha$ is the steepness of the sigmoid.
// ]

// #let b2 = std-block(breakable: false)[
//   #box(
//     fill: theme.sapphire.lighten(10%),
//     outset: 1mm,
//     inset: 0em,
//     radius: 3pt,
//   )[#text(white, size: 12pt, font: "JetBrainsMono NFM")[Indicator]] \
//   $
//     I = exp(-((S - 1)^2) / (2 sigma^2)) 
//   $ <branch_indicator2>
//   where 
//   - $I$ is the indicator function, 
//   - $S$ is the neighbour sum obtained by convolution with $k$ from @branch_kernel, 
//   - $sigma$ is a scalar value.
// ]

// #let b3 = std-block(breakable: false)[
//   #box(
//     fill: theme.sapphire.lighten(10%),
//     outset: 1mm,
//     inset: 0em,
//     radius: 3pt,
//   )[#text(white, size: 12pt, font: "JetBrainsMono NFM")[Pixel endpoint contribution]] \
//   $
//     f(p) = p dot I 
//   $ <branch_endpoint>
//   where 
//   - $p$ comes from @soft_threshold,
//   - $I$ is the indicator from @branch_indicator2.
// ]

// #let b4 = std-block(breakable: false)[
//   #box(
//     fill: theme.sapphire.lighten(10%),
//     outset: 1mm,
//     inset: 0em,
//     radius: 3pt,
//   )[#text(white, size: 12pt, font: "JetBrainsMono NFM")[Loss function definition]] \
//   $
//     cal("L")_"branch" = (E - T)^2
//   $ <branch_loss>
//   where 
//   - $E$ is the number of endpoints,
//   - $T$ is the target number of endpoints.
// ]

// #grid(
//   columns: (1fr, 1fr),
//   b1, b2,
//   b3, b4
// )

// Several calculation contribute to the total derivative. Firstly, the derivative of the loss in @branch_loss is simply
// $
//   (partial cal("L")_"branch")/(partial E) = 2  (E - T)
// $ <branch_dedt2>

// Secondly, the direct contribution from the indicator function is found by taking the derivative of @branch_indicator2 with respect to the pixel value $p$:
// $
//   partial/(partial p) p dot I = I
// $
// directly representing the sensitivity of @branch_endpoint. Thirdly, since the indicator $I$ is a function of the neighbour sum $S$, which in turn depends on $p$ via convolution, there is an indirect contribution as well. Given @branch_indicator2, its derivative with respect to $S$ is 
// $
//   (partial I)/(partial S) = - (S-1)/(sigma^2) dot I
// $ <branch_indirect>
// To then find the contributions from neighbouring pixels, the product $p dot$ @branch_indirect is convolved with the kernel $k$ from @branch_kernel. Lastly, the derivative of @soft_threshold is found by taking the derivative of $p$ with respect to $x$. First, the derivative of the sigmoid function is found:
// $
//   sigma'(x) = sigma(x)  (1 - sigma(x))
// $
// So the derivative of $p$ with respect to input $x$ is 
// $
//   (partial)/(partial x) p = alpha/255 sigma(alpha (x/255 - t)) (1 - sigma(alpha (x/255 - t)))
// $ <branch_sigmoid>

// Now, to find the total gradient, the chain rule will be applied to find the gradient of the loss with respect to the input $x$:
// $
//   (partial)/(partial x) cal("L")_"branch" = (partial cal(L)_"branch")/(partial E) dot (partial E)/(partial p) dot (partial p)/(partial x)
// $ <branch_grad>
// - $(partial cal(L)_"branch")/(partial E)$ was found in @branch_dedt2,

// - $(partial E)/(partial p)$ is the direct and indirect contributions from the indicator function, and

// - $(partial p)/(partial x)$ is @branch_sigmoid rewritten as $alpha/255 p(1-p)$

// yielding the complete gradient
// $
//   (partial)/(partial x) cal("L")_"branch" = 2  (E - T) dot [ I + "Conv"(p dot (- (S-1)/(sigma^2))) ] dot alpha/255 p(1-p) 
// $ <complete_branch_grad>
// as seen in @code:branching_loss line 48. Some results can be seen in @fig:loss_topo_branch.

// #std-block(breakable: false)[
//   #v(-1.2em)
//   #box(
//     fill: theme.sapphire,
//     outset: 0em,
//     inset: 0em,
//   )
//   #figure(
//   image("../../../figures/img/loss_example/branch_loss_plots_report.png", width: 100%),
//   caption: [ Results of using the branching part of the topology loss. ]
//   ) <fig:loss_topo_branch>
// ]


// // considerations: It should only penalize branches that are dead-ends, not the points at which it branches. If deg(p) > 2, that shows the points at which it is branching, but this is not critical as path thickness is not to be considered in this part of the loss function. So only penalize end-points that bring the total above 2.


// ==== Entry/Exit <c4:topo_entry>