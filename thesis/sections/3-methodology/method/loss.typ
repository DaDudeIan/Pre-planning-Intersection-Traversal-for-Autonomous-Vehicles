#import "../../../lib/mod.typ": *

== Loss Function Design <c4:loss>

// Bare bones loss function: dot product
// 
// class imbalance (a lot more background pixels than path pixels)
// - Binary Cross Entropy (BCE). Binary segmentation, pixel-wise classification.
// - Dice loss. Overlap between the predicted and ground truth masks. (https://paperswithcode.com/paper/generalised-dice-overlap-as-a-deep-learning)
// - IoU loss. Intersection over Union. Differential approximation (soft IoU loss)
// - Focal loss.
// 
// Distance based
// - Hausdorff distance. Maximum distance between two sets. Penalizes outliers.
// - Chamfer distance. Average distance between two sets.
// 
// Smoothness
// - Heavily penalize breaks. Topology analysis.
// - Total Variation (TV) loss. Penalize sharp changes.
// 
// Dot product
// + BCE or Dice
// + Distance based (Hausdorff, Chamfer)
// + Topology/Smoothness
// 
// For diffusion: combine with standard diffusion loss (MSE)

One of the most critical parts of designing a deep learning model, is the creation of the loss function that will guide the training. The loss function is a measure of how well the model is performing, and it is used to adjust the model's parameters during training. Therefore, the choice of loss function is crucial to the success of the model. In this section, I will discuss the design of the loss functions used to train the models I've set out to create. It will consist of a combination of different loss functions, each designed to capture different aspects of the problem at hand. Firstly, I will cover the development of the novel "cold map"-based part of the loss function, which is supposed to guide the model by penalizing points further away from the true path subject to some threshold. Secondly, I will discuss the use of a commonly used loss function, the #acr("BCE") loss, which is used for binary segmentation tasks. Finally, the last part of the loss function will deal with the actual topology of the predicted path. It will heavily penalize breaks in the path, branches in paths, and other topological errors, such as not reaching connecting the entry and exit. With these three different loss functions, the goal is to compare different combinations of them to determine which one works best for the task at hand. 

=== Cold Map Loss <c4:cold_loss>

The first part of the loss function is the cold map loss. This involves using the predicted path generated by the model, and comparing it to the cold map from the dataset. The creation of the cold maps are detailed in @c4:cold_maps. Briefly, the cold maps are grids of the same size as the input image, where the intensity of each cell is a value derived from the distance to the nearest path pixel magnified beyond some threshold. 

The main idea behind the cold map loss is to introduce spatial penalty that increases as the distance from the true path increases. Though it is similar to #acr("BCE"), it differs in some key aspects. It is not pixel-wise, but rather a global loss that is calculated over the entire image. This means that slight deviations from the true path are penalized less than those with a larger discrepancy. This property of the loss function is a desirable trait for path-planning tasks, as minor offsets from the true path are less critical than larger ones. This loss function is defined as follows:

$
  cal("L") = sum_(i=1)^H sum_(j=1)^W C_(i j) P_(i j)
$ <eq:loss_cold>

where $C_(i j)$ is the cold map value at pixel $(i, j)$, and $P_(i j)$ is the predicted path value at pixel $(i, j)$. This version of the loss function is a simple dot product between the cold map and the predicted path. Thus, after flattening the cold map and the predicted path matrices, the loss is calculated as the dot product between the two vectors: 

$
  cal("L")_"cold" = C dot P
$ <eq:loss_cold_flat>

where $C$ is the cold map vector and $P$ is the predicted path vector, giving a scalar value, contributing to the total loss. This value does, however, grow extremely quickly, as the dot product, as shown by @eq:loss_cold, is simply a sum over the entire image. This means that the loss value will be very high, even for small deviations from the true path, and extremely high for large deviations or noisy images, as expected from the model during early stages of training. While this rapid growth can be combated by introducing a very low weight to the loss function, doing so would also mean that smaller deviations become irrelevant, which in undesired. Thus, inspired by BCE, I will introduce a mean reduction to the loss function, which will divide the loss by the number of pixels in the image. This will ensure that the loss value is more stable and that the model can learn from smaller deviations. With this, the implementation of a cold map loss, will be based on the following equation:

$
  cal("L")_"cold" = 1 / (H dot W) sum_(i=1)^H sum_(j=1)^W C_(i j) P_(i j) 
$ <eq:loss_cold_mean>

where $H$ and $W$ are the height and width of the image, respectively. The implementation of @eq:loss_cold_mean is very straightforward, and is shown in the code listing below:

#listing([
  ```python
def cmap_loss(cmap_gt, path_pred, reduction) -> torch.Tensor:
  cmap_f = torch.flatten(cmap_gt)
  path_f = torch.flatten(path_pred)
  
  loss = torch.dot(cmap_f, path_f)
    
  return loss if reduction != 'mean' else loss / len(cmap_f)

class CmapLoss(nn.Module):
  def __init__(self, weight: float = 1.0, reduction: str = 'mean'):
    super(CmapLoss, self).__init__()
    self.weight = weight
      
  def forward(self, cmap_gt: torch.Tensor, path_pred: torch.Tensor):
    loss = cmap_loss_torch(cmap_gt, path_pred, self.reduction)
    return self.weight * loss
  ```
],
caption: [Implementation of the cold map loss calculation using PyTorch.]
) <code.cold_loss>

To implement this loss function, a class `CmapLoss` is created, which inherits from `torch.nn.Module`. The class has a single parameter, `weight`, which is used to scale the loss value. The `forward` method takes the ground truth cold map `cmap_gt` and the predicted path `path_pred` as inputs. The method then calculates the loss using the `cmap_loss` function and scales it by the `weight` parameter. The function `cmap_loss` takes two PyTorch tensors as inputs: `cmap_gt`, which represents the ground truth cold map, and `path_pred`, which is the predicted path output from the model. The cold map and predicted path are initially matrices with dimensions corresponding to the image's height and width. To compute the loss as a single scalar, both matrices are flattened into one-dimensional vectors. The function then calculates the dot product between these two vectors using `torch.dot`, effectively summing the element-wise products. If the `reduction` parameter is set to `mean`, the loss is divided by the number of elements in the vectors, which is the total number of pixels in the image. 

Examples of this loss function in action is shown in @fig:cmap_loss_comp. The left and center top row plots overlays a complete, single width path on top of the cold map. This highlights the fact that paths that are close to the true path, are penalized less and the further away, the more the penalty explodes in value. The last image in the top row shows how the loss handles a noisy image. As it can be seen, the loss is significantly higher than the rest. This is a desired trait of the loss function, as noise is just about the exact opposite of a continuous path. The bottom row shows alternate paths. The path on the rightmost image is the true path, which shows that if the path is dead on, then the penalty is none. The leftmost image, shows a path that is not connected. As shown, it still scores a perfect score. This is because the cold map loss only penalizes the distance from the path, not the topology of the path itself, so breaks will only result in a higher score. Lastly, the center image shows the true path, but with several branches. As seen in the loss value, this also incurs very little penalty. The bottom row of images highlight a dire need for a topology-based loss, which will be explored in @c4:topology_loss. 

#std-block(breakable: false)[
  #v(-1em)
  #box(
    fill: theme.sapphire,
    outset: 0em,
    inset: 0em,
  )
  #figure(
  image("../../../figures/img/loss_example/cmap_loss_comparison7.png", width: 100%),
  caption: [Paths drawn on top of a cold map with their associated loss above calculated using the function from @code.cold_loss. The top row shows fully connected paths, while the bottom row shows paths with breaks and branches, as well as the true path.]
) <fig:cmap_loss_comp>
]

=== Binary Cross-Entropy Loss <c4:bce_loss>

The #acr("BCE") loss is a commonly used loss function for binary segmentation tasks, which is relevant for the task at hand due to the pixel-subset nature of the problem, i.e. pixels can be either 1 or 0. Furthermore, it is well-versed in handling heavily imbalanced data, which is the case when dealing with classification tasks where one class is much more prevalent than the other, like path and non-path pixels in an image. These properties make it ideal for this problem, as the background pixels are much more prevalent than the path pixels. The implementation is using the definition from PyTorch #footnote([Full implementation details can be found in the official documentation: https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html]), which is defined as follows:


$
  cal("l")(x,y) = L = {l_1, dots , l_N}^T
$ <eq:bce_loss>
with
$
  l_n = - w_n [y_n log x_n + (1 - y_n) log(1 - x_n)]
$ <eq:bce_loss_n>
where $w_n$ is a weight parameter, $y_n$ is the ground truth label, $x_n$ is the predicted label, and $cal("l")$ is subject to
$
  cal("l")(x,y) = cases(
    #align(right)[#box[$"mean"(L)$], #h(5mm)]& "if reduction = 'mean'",
    #align(right)[#box[$"sum"(L)$],]& "if reduction = 'sum'"
  )
$ <eq:bce_loss_reduction>
depending on the reduction parameter. The left-hand side of @eq:bce_loss_n is activated when the ground truth label is $1$. It evaluates how well the positive class's predicted probability $x_n$ aligns with the ground truth. A smaller loss is achieved when the predicted probability is close to $1$. The right-hand side of @eq:bce_loss_n is activated when the ground truth label is $0$. It evaluates how well the negative class's predicted probability $x_n$ aligns with the ground truth. A smaller loss is achieved when the predicted probability is close to $0$. The BCE loss is then calculated as the sum or mean of the individual losses, depending on the `reduction` parameter. The weight parameter $w_n$ can be used to scale the output of the loss function. #acr("BCE") quantifies the dissimilarity between the predicted probabilities and the actual labels, giving a sense of how well the model is performing. For example, calculating the BCE with $y=1$ and $x=0.8$ gives

#let nonum(eq) = math.equation(block: true, numbering: none, eq)
#nonum($-1 dot (1 dot log(0.8) + (1 - 1) dot log(1 - 0.8)) = -log(0.8) = 0.223$)

This value represents the dissimilarity between the predicted probability of $0.8$ and the actual label of $1$. A lower value indicates that the model's prediction is closer to the ground truth, suggesting a more accurate classification. Alternatively, the value can be near $1$, indicating a large discrepancy between the predicted and actual labels. So, a value of $0.223$ is a good result, as it indicates that the model is performing well, with some room for improvement. For contrast, if the predicted label is $0.4$, but the true label is $1$, the dissimilarity would be $-log(0.4) approx 0.916$. This higher value reflects a more significant error in prediction. From this, note that the function calculates the dissimilarity for both positive and negative classes. So, in the case of the predicted label being $0.4$ and the true label being $0$, the loss value would be much better at just $-log(1-0.4) approx 0.51$. 

In the PyTorch implementation, the BCE loss can be either summed or averaged over the batch, depending on the `reduction` parameter. While using the sum method can provide stronger signals on rare but critical pixels, averaging might help maintain stability across batches, especially when dealing with very imbalanced datasets. Thus, the mean method is chosen for this project, as it leads to a more stable training process with smaller fluctuations in the loss values. The BCE loss is calculated using the following code snippet:

#listing([
  ```python
def bce_loss_torch(path_gt, path_pred, reduction) -> torch.Tensor:
    bce = torch.nn.BCELoss(reduction=reduction)

    bce_loss = bce(path_pred, path_gt)
    
    return bce_loss

class BCELoss(nn.Module):
  def __init__(self, weight: float = 1.0):
    super(BCELoss, self).__init__()
    self.weight = weight
      
  def forward(self, path_gt: torch.Tensor, path_pred: torch.Tensor, reduction: str = 'mean'):
    loss = bce_loss_torch(path_gt, path_pred, reduction)
    return self.weight * loss
  ```
],
caption: [Implementation of the #acr("BCE") loss function using PyTorch.]
) <code.bce_loss>

To implement the BCE loss function, a smaller wrapper class for the existing PyTorch implementation is created. The class `BCELoss` inherits from `torch.nn.Module` and has a single parameter, `weight`, which is used to scale the loss value. The `forward` method takes the ground truth path `path_gt`, the predicted path `path_pred`, and the reduction method as inputs. The method then calculates the loss using the `bce_loss_torch` function and scales it by the `weight` parameter. The function `bce_loss` calculates the BCE loss between the ground truth and predicted paths. It takes the paths to the ground truth and predicted images as input, reads them using OpenCV, and converts them to PyTorch tensors after normalization, since the images are stored as 8-bit greyscale images. The function creates a BCE loss `criterion` using `torch.nn.BCELoss` and calculates the loss using the ground truth and predicted tensors. The loss value is then returned as a float. In the tensor version of the function, the paths are already tensors, and the function can be called directly with the tensors as input.

#std-block(breakable: false)[
  #v(-1em)
  #box(
    fill: theme.sapphire,
    outset: 0em,
    inset: 0em,
  )
  #figure(
  image("../../../figures/img/loss_example/cmap_loss_comparison4.png", width: 100%),
  caption: [The ground truth #ball("#E4B96D") compared to some drawn path #ball(white). The losses above the plots are the BCE loss using function in @code.bce_loss using `mean` as the string passed as the reduction method. ]
) <fig:bce_loss_comp>
]

Examples of the `mean`-based BCE loss is shown in @fig:bce_loss_comp, showing various interesting aspects of the loss function. (b) shows an expectedly high loss value, as the path is far from the true path. This matches the case for the cold map loss. (a), (c), and (d) show very similar loss values, despite being vastly different, both in terms of closeness to the path, but also where they are going to and from, further highlighting the need for a topology-based loss. Even their sum counterparts show very similar values. Interestingly, the losses seen in (f) and (h) are very different, when the cold map based loss showed them as being equal. This shows that BCE is more sensitive to the topology of the path, but as (g) shows, it still gives a very low loss value when a path has branches. All of this shows that some topology analysis is needed for the loss function to capture realistic paths.

Other considerations for handling imbalanced data include methods like Dice @dice_loss similarity coefficient and Focal loss @focal_loss, which can also be effective in certain contexts. The Dice similarity coefficient is a measure of overlap between two samples, and is particularly useful when dealing with imbalanced data. The Focal loss is designed to address the class imbalance problem by focusing on hard examples that are misclassified. These methods can be used in conjunction with the BCE loss to improve the model's performance, especially when dealing with heavily imbalanced datasets. But for the purposes of this project, the BCE loss is expected to be sufficient to handle the class imbalance.




=== Topology Loss <c4:topology_loss>

// @topo1 also highlights the use of BCE in combination with a topology loss.
// Mention TopoLoss from TopoNets, but that the smoothing is not what is needed here.
// 

As the previous sections have highlighted, there is a dire need for a topology-based loss function. The cold map loss and the BCE loss are both excellent at penalizing paths that are far from the true path, but they do not penalize breaks in the path or ensure that the predicted path is continuous. This is where the topology loss comes in. The topology loss function will revolve around getting the output from the model to be a singular, continuous components, meaning it does not contain breaks or holes. Formally, this is done by aiming for specific Betti number values, ensuring that the predicted path has a single connected component and no loops. The details of this approach are discussed in the following sections.

Considerations of using existing topology existing methods. Dep #etal @topoloss introduced TopoNets and TopoLoss. This loss function revolves around penalizing jagged paths and encouraging smooth, brain-like topographic organization within neural networks by reshaping weight matrices into two-dimensional cortical sheets and maximizing the cosine similarity between these sheets and their blurred versions. Cortical sheets are two-dimensional grids formed by reshaping neural network weight matrices to emulate the brain's spatial organization of neurons, enabling topographic processing. While initially interesting in the context of this project, simple testing showed that the values returned from this loss, did not give a proper presentation of the path's topology, outside of its smoothness. And while smoothness is a part of the topology, this will largely be handled by the #acr("BCE") loss.

==== Continuity <c4:topo_cont>

This section will present the topology-based loss function designed for this project, with a focus on ensuring that the predicted path is continuous and does not contain any breaks or holes. This is a crucial aspect of the task at hand, as the goal is to create a path that a vehicle can follow. Breaks in a path would be unrealistic for a grounded vehicle to follow. As a starting point, it is important to understand the concept of Betti numbers:

#std-block(breakable: true)[
  #box(
    fill: theme.sapphire.lighten(10%),
    outset: 1mm,
    inset: 0em,
    radius: 3pt,
  )[#text(white, size: 12pt, font: "JetBrainsMono NFM")[Betti Numbers]] \
  Betti numbers @betti come from algebraic topology, and are used to distinguish topological spaces based on the connectivity of $n$-dimensional simplicial complexes. The $n$th Betti number, $beta_n$, counts the number of $n$-dimensional holes in a topological space. The Betti numbers for the first three dimensions are:
   - $beta_0$: The number of connected components.
   - $beta_1$: The number of loops.
   - $beta_2$: The number of voids.
   The logic follows that, in 1D, counting loops are not possible, as it is simply a line. This, if the number is greater than 1, it means it is split into more than component. In 2D, the number of loops is counted, i.e. a circled number of pixels. In 3D, this extends to voids. 
 ]

With this, for the 2D images used in this project, the Betti numbers are $beta_0$ and $beta_1$. The topology loss is designed to ensure that the predicted path has a single connected component and no loops. This is achieved by aiming for the Betti numbers to be $beta_0 = 1$ and $beta_1 = 0$. Higher dimensional Betti numbers are not relevant for this project, as the images are 2D. While Betti numbers are a powerful tool for topology analysis, they are not directly applicable to the loss function as they are discrete values. This means that offer no gradient information, which is essential for training a neural network. Instead, persistent homology is deployed.

#acr("PH") is a mathematical tool used to study topological features of data. Homology itself is a branch of algebraic topology concerned with procedures to compute the topological features of objects. Persistent homology extends the basic idea of homology by considering not just a single snapshot of a topological space but a whole family of spaces built at different scales. Instead of calculating Betti numbers for one fixed space, a filtration is performed. This filtration is a sequence of spaces, where each space is a subset of the next, i.e. a nested sequence of spaces where each one is built by gradually growing the features by some threshold. As this threshold varies, topological features such as connected components and loops will appear (be born) and eventually merge or vanish (die).

This birth and death of features is recorded in what is known as a persistence diagram or barcode (See @fig:persistent_homology). In these diagrams, each feature is represented by a bar (or a point in the diagram) whose length indicates how persistent, or significant, the feature is across different scales. Features with longer lifespans are generally considered to be more robust and representative of the underlying structure of the data, whereas those that quickly appear and disappear might be attributed to noise.
// // possibility of exploding loss when noisy image


#let fig1 = { image("../../../figures/img/loss_example/comp12.png", width: 80%) }
#let fig2 = { image("../../../figures/img/loss_example/pers_bar.png") }
#let fig3 = { image("../../../figures/img/loss_example/pers_diag.png") }

// #let fig4 = { image("../../../figures/img/loss_example/comp7.png", width: 80%) }
// #let fig5 = { image("../../../figures/img/loss_example/pers_bar_2.png") }
// #let fig6 = { image("../../../figures/img/loss_example/pers_diag_2.png") }

#std-block(breakable: false)[
  #figure(
    grid(
    columns: (1fr, 1fr, 1fr),
    column-gutter: 1mm,
    align: (center, center),
    fig1, fig2, fig3,
    //fig4, fig5, fig6
    ),
    caption: [The top row shows a connected path along with its persistence barcode and persistence diagram, while the bottom row shows a disconnected path. The number of lines in the barcode, stems from the fact that the images are rather large in size and thus the number of built spaces are many.]
  ) <fig:persistent_homology>
]

The following will cover the method used to achieve the topology-based loss function. It closely follows the work done by Clough #etal @topology_loss with some minor changes that will be pointed out. Furthermore, the implementation is done using the Gudhi library, which is a Python library for computational topology. It provides a set of tools for computing persistent homology and other topological features of data. The library is designed to be efficient and easy to use, making it a good choice for this project. Lastly, PyTorch's ability to create custom autograd functions is used to implement the persistent homology loss function.

The output from the network is a tensor of size 
$ Omega = H times W $ 
where, in this project, $H=400$ and $W=400$. Then for each pixel $x in Omega$, the network outputs a logit vector:
$ L(x) in RR^C $
where $C$ is the number of classes. A softmax function is applied to give class probabilities:
$
  P_c (x)  = (e^(L_c(x)))/(sum_(j=1)^C e^(L_j(x)))
$

Here it is noted that the background class is $"bg" = 0$. Following that, a foreground probability map is created by removing the background probabilities:
$
  "fg"(x) = 1 - P_"bg" (x) in [0,1]
$  

Now, persistent homology on images is usually computed through sub-level sets. Clough #etal formulates PH on super-level sets, which is the opposite of sub-level sets. Sub-level sets were chosen, as the alternative wouldn't work in practice, as Gudhi is kept in sub-level mode. Therefore, an inverted version of the foreground probability map is created:
$
  f(x) = 1 - "fg"(x) in [0,1]
$
where low values mark confident foreground pixels, and mark confident background pixels. The filtration, as generated inside Gudhi, is defined as 
$
  K_0 subset.eq K_(t_1) subset.eq dots.h.c subset.eq K_1
$
where each sub-level set is
$
  K_t = {x in Omega | f(x) <= t}
$ 
This happens inside the `CubibalComplex` function, which is a Gudhi function that creates a cubical complex from the input. In short, the cubical complex is a data structure that represents the topological features of the input data. The Gudhi library is used to compute the persistent homology of the cubical complex, which is then used to calculate the Betti numbers. 

From the cubical complex, persistence pairs are found, which are the birth and death values of the topological features. Gudhi returns a multiset for each homology dimension:
$
  D_k (f) = {(b_i^((k)), d_i^((k))) }
$
with the constraint $0 <= b_i^((k)) < d_i^((k)) <= infinity$, where $k=0$ for connected components and $k=1$ for loops. The component whose death time is $+infinity$ is discarded, as it is not influenced by the other pixels' values and all other pairs have finite deaths.

At this stage, some extra work is required, as Gudhi also returns _where_ the feature pairs are born and die. In other words, flat Fortran-order indices of the pixels whose grey-values equal the birth and death values. These indices are remapped to PyTorch's row-major order with a small helper function:
$
  "_F2C"(n) = r + c H
$
with $(r, c) = "divmod"(n, H)$ where $"divmod"$ returns the element-wise quotient and remainder of the two inputs. 

Finally, the actual loss is found by defining the persistence, ignoring bars that are shorter than some threshold $epsilon$:
$
  "pers"_i^((k)) = d_i^((k)) - b_i^((k)) > epsilon
$
where the threshold $epsilon$ is typically a low value. Thus, the per-image loss, i.e. each image in a batch, is defined as:
$
  cal(L)(f) = w_0 sum_((b,d) in D_0(f), d-b > epsilon) (d-b)^p + w_1 sum_((b,d) in D_1(f), d-b > epsilon) (d-b)^p
$ <eq:loss_topo>
where $w_0$ and $w_1$ are weights for the two dimensions, and $p$ is a power parameter. The loss is then averaged over the batch size:
$
  "Loss" = 1/B sum_(b=1)^B cal(L)(f_b)
$
Because each summand in @eq:loss_topo is built from pixel values $f(x)$, gradients propagate exactly to those birth- and death-pixels:

$
  partial / (partial f(x)) (d-b)^p = cases(
    #align(right)[#box[$-p(d-b)^(p-1)$], #h(5mm)]& "if" x "is the birth pixel,",
    #align(right)[#box[$p(d-b)^(p-1)$],]& "if" x "is the death pixel,",
    #align(right)[#box[$0$],]& "otherwise"
  )
$
This comes from the fact that the partial derivatives are immediate, as the summand for any bar depends only on the two scalar values from $f$.

Examples of the PH-based loss function is shown in @fig:loss_topo_cont. The far left image shows the output from a model, showing clear and separated components. This is the highly undesired trait this loss is designed to penalize. The loss value is $1.35$, which is a very high value. The next image shows the same model after a backwards pass has been made by the loss function. Already, it is clear to see that the model has improved, as the components are now largely connected, with only some stragglers. Then after another two iterations, the model is nearing a loss of $0$. The last image has a near $0$ loss after just another iteration. 

This highlights the effectiveness of this loss function. If this keeps going, the model will eventually reach a loss of $0$. However, this rudimentary testing of the loss function also highlights the fact that it alone does not care for class labels. After just 20 iterations, it would label every pixel as the same class, as it is only concerned with the topology of the path. This is a trait of the loss function that is not desired, but when it works in conjunction with the other loss functions, it gets balanced to create more accurate results. This will be covered in depth in @c4:training-strategy where the training strategy is presented.

#let fig1 = { image("../../../figures/img/loss_example/output0.png") }
#let fig2 = { image("../../../figures/img/loss_example/output1.png") }
#let fig3 = { image("../../../figures/img/loss_example/output2.png") }
#let fig4 = { image("../../../figures/img/loss_example/output3.png") }
#let fig5 = { image("../../../figures/img/loss_example/output4.png") }

#std-block(breakable: false)[
  #figure(
    grid(
      columns: (1fr, 1fr, 1fr, 1fr),
      fig1, fig2, fig4, fig5
    ),
  caption: [ Results of refining trained model a few iterations on the same image. ]
  ) <fig:loss_topo_cont>
]

